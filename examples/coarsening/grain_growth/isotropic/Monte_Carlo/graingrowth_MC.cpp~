// graingrowth.hpp
// Algorithms for 2D and 3D isotropic Monte Carlo grain growth
// Wright, Steven A., et al. "Potts-model grain growth simulations: Parallel algorithms and applications." SAND Report (1997): 1925.
// Questions/comments to tany3@rpi.edu (Yixuan Tan)

#ifndef GRAINGROWTH_UPDATE
#define GRAINGROWTH_UPDATE
#include <iomanip>
#include <vector>
#include <cmath>
#include <ctime>
#include <cstdio>
#include <cstdlib>
#include <cassert>
#include <pthread.h>
#include "rdtsc.h"
#include"graingrowth_MC.hpp"
#include"MMSP.hpp"
#include"tessellate.hpp"
#include"output.cpp"

/*-------------------------------------------------------------------
-----------------Define Constants for Monte Carlo--------------------
-------------------------------------------------------------------*/
// From Monte Carlo Simulation: L = lambda*K1*(t_mc)^(n1)
double lambda = 1e-3/1000;  // lambda = length/lattice_size
double K1 = 0.94514608;  
double n1 = 0.48921977;  

// From Experiments: L^n - (L_0)^n= K*t*exp(-Q/R/T)

double L0 = 1.1e-6;
double Q = 1.413e5; //fitted from Gangulee, A. ”Structure of electroplated and vapordeposited copper films. III. Recrystallization and grain growth.” Journal of Applied Physics 45.9 (1974): 3749-3756.
double n = 2.0; // from Gangulee, A. ”Structure of electroplated and vapordeposited copper films. III. Recrystallization and grain growth.” Journal of Applied Physics 45.9 (1974): 3749-3756
double K = 3.554e-5; //fitted from Gangulee, A. ”Structure of electroplated and vapordeposited copper films. III. Recrystallization and grain growth.” Journal of Applied Physics 45.9 (1974): 3749-3756.
double R = 8.314;

// Provide initial grain size information 
double L_initial = 2e-6; // for example, when using 1000X1000 grid to simulate a 2D 1mmx1mm film, inital grain radius of 1 voxels leads to 2e-6 m initial grain size in simulation.

void print_progress(const int step, const int steps, const int iterations);

namespace MMSP
{
template <int dim>
unsigned long generate(MMSP::grid<dim,int >*& grid, int seeds, int nthreads)
{
	#if (defined CCNI) && (!defined MPI_VERSION)
	std::cerr<<"Error: MPI is required for CCNI."<<std::endl;
	exit(1);
	#endif
	#ifdef MPI_VERSION
	int np = MPI::COMM_WORLD.Get_size();
	#endif

	unsigned long timer=0;
	if (dim == 2) {
		const int edge = 1000;
		int number_of_fields(seeds);
		if (number_of_fields==0) number_of_fields = static_cast<int>(float(edge*edge)/(M_PI*1*1));// Average grain is a disk of radius 1 voxels
		#ifdef MPI_VERSIONacc
		while (number_of_fields % np) --number_of_fields; 
		#endif
		grid = new MMSP::grid<dim,int>(0, 0, edge, 0, edge);

		#ifdef MPI_VERSION
		number_of_fields /= np;
		#endif

		#if (!defined MPI_VERSION) && ((defined CCNI) || (defined BGQ))
		std::cerr<<"Error: CCNI requires MPI."<<std::endl;
		std::exit(1);
		#endif
		timer = tessellate<dim,int>(*grid, number_of_fields, nthreads);
		#ifdef MPI_VERSION
		MPI::COMM_WORLD.Barrier();
		#endif
	} else if (dim == 3) {
		const int edge = 128;
		int number_of_fields(seeds);
		if (number_of_fields==0) number_of_fields = static_cast<int>(float(edge*edge*edge)/(4./3*M_PI*5.*5.*5.)); // Average grain is a sphere of radius 10 voxels
		#ifdef MPI_VERSION
		while (number_of_fields % np) --number_of_fields;
		#endif
		grid = new MMSP::grid<dim,int>(0, 0, edge, 0, edge, 0, edge);

		#ifdef MPI_VERSION
		number_of_fields /= np;
		#endif

		timer = tessellate<dim,int >(*grid, number_of_fields, nthreads);
		#ifdef MPI_VERSION
		MPI::COMM_WORLD.Barrier();
		#endif
	}

/*------------------Initial tmc----------------------*/
  double tmc_initial = pow(L_initial/K1/lambda,1.0/n1);
  vector<int> coords (dim,0);
  double temp[2] = {673.0, 673.0}; // define temperatures at two ends of x direction 
  if(dim==2){
    for(int codx=x0(*grid, 0); codx <= x1(*grid, 0); codx++) 
      for(int cody=x0(*grid, 1); cody <= x1(*grid, 1); cody++){
        coords[0] = codx;
        coords[1] = cody;
        (*grid).AccessToTmc(coords) = tmc_initial;
        (*grid).AccessToTmp(coords) = temp[1]+(temp[0]-temp[1])/1000*codx; //set the initial temp, 1000 is the lattice size
      }
  }
  else if(dim==3){
    for(int codx=x0(*grid, 0); codx <= x1(*grid, 0); codx++) 
      for(int cody=x0(*grid, 1); cody <= x1(*grid, 1); cody++) 
        for(int codz=x0(*grid, 2); codz <= x1(*grid, 2); codz++){
          coords[0] = codx;
          coords[1] = cody;
          coords[2] = codz;
          (*grid).AccessToTmc(coords) = tmc_initial;
          (*grid).AccessToTmp(coords) = temp[1]+(temp[0]-temp[1])/128*codx; //set the initial temp, 128 is the lattice size
        }
  }
/*---------------------------------------------------*/
	return timer;
}

unsigned long generate(int dim, char* filename, int seeds, int nthreads)
{
	#if (defined CCNI) && (!defined MPI_VERSION)
	std::cerr<<"Error: MPI is required for CCNI."<<std::endl;
	exit(1);
	#endif
	int rank=0;
	#ifdef MPI_VERSION
	rank = MPI::COMM_WORLD.Get_rank();
	#endif

	unsigned long timer = 0;
	if (dim == 2) {
		MMSP::grid<2,int>* grid2=NULL;
		timer = generate<2>(grid2,seeds,nthreads);
		assert(grid2!=NULL);
		#ifdef BGQ
		output_bgq(*grid2, filename);
		#else
		output(*grid2, filename);
		#endif
		#ifndef SILENT
		if (rank==0) std::cout<<"Wrote initial file to "<<filename<<"."<<std::endl;
		#endif
	}

	if (dim == 3) {
		MMSP::grid<3,int>* grid3=NULL;
		timer = generate<3>(grid3,seeds,nthreads);
		assert(grid3!=NULL);
		#ifdef BGQ
		output_bgq(*grid3, filename);
		#else
		output(*grid3, filename);
		#endif
		#ifndef SILENT
		if (rank==0) std::cout<<"Wrote initial file to "<<filename<<"."<<std::endl;
		#endif
	}
	return timer;
}

template <int dim> struct flip_index {
	MMSP::grid<dim, int>* grid;
  int num_of_cells_in_thread;
	int sublattice;
  int num_of_points_to_flip;
  int cell_coord[dim];
  int lattice_cells_each_dimension[dim];
  double Pdenominator;
};

template <int dim> void* flip_index_helper( void* s )
{
  srand(time(NULL)); /* seed random number generator */
	flip_index<dim>* ss = static_cast<flip_index<dim>*>(s);
	vector<int> x (dim,0);
  int first_cell_start_coordinates[dim];
  for(int kk=0; kk<dim; kk++) first_cell_start_coordinates[kk] = x0(*(ss->grid), kk);
  for(int i=0; i<dim; i++){
    if(x0(*(ss->grid), i)%2!=0) first_cell_start_coordinates[i]--;
  }
  int cell_coords_selected[dim];
	for (int hh=0; hh<ss->num_of_points_to_flip; hh++) {
	  // choose a random cell to flip
    int cell_numbering_in_thread = rand()%(ss->num_of_cells_in_thread); //choose a cell to flip, from 0 to num_of_cells_in_thread-1
    if(dim==2){
      cell_coords_selected[dim-1]=((ss->cell_coord)[dim-1]+cell_numbering_in_thread)%(ss->lattice_cells_each_dimension)[dim-1];//1-indexed
      cell_coords_selected[0]=(ss->cell_coord)[0]+(((ss->cell_coord)[dim-1]+cell_numbering_in_thread)/(ss->lattice_cells_each_dimension)[dim-1]);
    }else if(dim==3){
      cell_coords_selected[dim-1]=((ss->cell_coord)[dim-1]+cell_numbering_in_thread)%(ss->lattice_cells_each_dimension)[dim-1];//1-indexed
      cell_coords_selected[1]=(  (ss->cell_coord)[1]+ ((ss->cell_coord)[dim-1]+cell_numbering_in_thread)/(ss->lattice_cells_each_dimension)[dim-1]  )%(ss->lattice_cells_each_dimension)[1];
      cell_coords_selected[0]=(ss->cell_coord)[0]+ ( (ss->cell_coord)[1] + ((ss->cell_coord)[dim-1]+cell_numbering_in_thread)/(ss->lattice_cells_each_dimension)[dim-1] ) /(ss->lattice_cells_each_dimension)[1];
    }
    for(int i=0; i<dim; i++){
      x[i]=first_cell_start_coordinates[i]+2*cell_coords_selected[i];
    }
    if(dim==2){
      switch(ss->sublattice){
        case 0:break;// 0,0
        case 1:x[1]++; break; //0,1
        case 2:x[0]++; break; //1,0
        case 3:x[0]++; x[1]++; break; //1,1
      }
    }else if(dim==3){
      switch(ss->sublattice){
        case 0:break;// 0,0,0
        case 1:x[2]++; break; //0,0,1
        case 2:x[1]++; break; //0,1,0
        case 3:x[2]++; x[1]++; break; //0,1,1
        case 4:x[0]++; break; //1,0,0
        case 5:x[2]++; x[0]++; break; //1,0,1
        case 6:x[1]++; x[0]++; break; //1,1,0
        case 7:x[2]++; x[1]++; x[0]++; //1,1,1
      }
    }

    bool site_out_of_domain = false;
    for(int i=0; i<dim; i++){
      if(x[i]<x0(*(ss->grid), i) || x[i]>=x1(*(ss->grid), i)){
//      if(x[i]<x0(*(ss->grid), i) || x[i]>x1(*(ss->grid), i)-1){
        site_out_of_domain = true;
        break;//break from the for int i loop
      }
    }
    if(site_out_of_domain == true){
      hh--;
      continue; //continue the int hh loop
    }

    double temperature=(*(ss->grid)).AccessToTmp(x);
    double t_mcs = (*(ss->grid)).AccessToTmc(x);
    double Pnumerator = exp(-Q/R/temperature)/pow(t_mcs,(n*n1-1));
    double site_selection_probability = Pnumerator/ss->Pdenominator;

	  double rd = double(rand())/double(RAND_MAX);
    if(rd>site_selection_probability) continue;//this site wont be selected
  
		int spin1 = (*(ss->grid))(x);
		// determine neighboring spins
    vector<int> r(dim,0);
    std::vector<int> neighbors;
    neighbors.clear();
    unsigned int number_of_same_neighours = 0;
    if(dim==2){
      for (int i=-1; i<=1; i++) {
        for (int j=-1; j<=1; j++) {
          if(!(i==0 && j==0)){
            r[0] = x[0] + i;
            r[1] = x[1] + j;
            if(r[0]<g0(*(ss->grid), 0) || r[0]>=g1(*(ss->grid), 0) || r[1]<g0(*(ss->grid), 1) || r[1]>=g1(*(ss->grid), 1) )// not periodic BC
              continue;// neighbour outside the global boundary, skip it. 
            int spin = (*(ss->grid))(r);
            neighbors.push_back(spin);
            if(spin==spin1) 
              number_of_same_neighours++;
          }
        }
      }
    }else if(dim==3){
		  for (int i=-1; i<=1; i++){
			  for (int j=-1; j<=1; j++){
			    for (int k=-1; k<=1; k++) {
            if(!(i==0 && j==0 && k==0)){
				      r[0] = x[0] + i;
				      r[1] = x[1] + j;
				      r[2] = x[2] + k;
              if(r[0]<g0(*(ss->grid), 0) || r[0]>=g1(*(ss->grid), 0) || r[1]<g0(*(ss->grid), 1) || r[1]>=g1(*(ss->grid), 1) ||
                 r[2]<g0(*(ss->grid), 2) || r[2]>=g1(*(ss->grid), 2))// not periodic BC
                continue;// neighbour outside the global boundary, skip it. 
				      int spin = (*(ss->grid))(r);
              neighbors.push_back(spin);
              if(spin==spin1) 
                number_of_same_neighours++;
            }
			    }
        }
		  }
    }

    //check if inside a grain
    if(number_of_same_neighours==neighbors.size()){//inside a grain
      continue;//continue for
    }
    //choose a random neighbor spin
    int spin2 = neighbors[rand()%neighbors.size()];
		// choose a random spin from Q states
		if(spin1!=spin2){
			// compute energy change
			double dE = 0.0;
      if(dim==2){
			  for (int i=-1; i<=1; i++){
				  for (int j=-1; j<=1; j++){
            if(!(i==0 && j==0)){
  					  r[0] = x[0] + i;
	  				  r[1] = x[1] + j;
	            if(r[0]<g0(*(ss->grid), 0) || r[0]>=g1(*(ss->grid), 0) || r[1]<g0(*(ss->grid), 1) || r[1]>=g1(*(ss->grid), 1) ) // not periodic BC
              continue;// neighbour outside the global boundary, skip it. 
    				  int spin = (*(ss->grid))(r);
				      dE += 1.0/2*((spin!=spin2)-(spin!=spin1));
            }// if(!(i==0 && j==0))
				  }
        }
      }
      if(dim==3){
			  for (int i=-1; i<=1; i++){ 
				  for (int j=-1; j<=1; j++){ 
    	      for (int k=-1; k<=1; k++){ 
              if(!(i==0 && j==0 && k==0)){
					      r[0] = x[0] + i;
					      r[1] = x[1] + j;
					      r[2] = x[2] + k;
                if(r[0]<g0(*(ss->grid), 0) || r[0]>=g1(*(ss->grid), 0) || r[1]<g0(*(ss->grid), 1) || r[1]>=g1(*(ss->grid), 1) ||
                   r[2]<g0(*(ss->grid), 2) || r[2]>=g1(*(ss->grid), 2)) // not periodic BC
                  continue;// neighbour outside the global boundary, skip it.
					      int spin = (*(ss->grid))(r);
					      dE += 1.0/2*(spin!=spin2)-(spin!=spin1);
              }
				    }
          }
        }
      }
			// attempt a spin flip
			double r = double(rand())/double(RAND_MAX);
      double kT = 1.3806488e-23*temperature;
			if (dE <= 0.0) (*(ss->grid))(x) = spin2;
  	  else if (r<exp(-dE/kT)) (*(ss->grid))(x) = spin2;
		}
	}
	pthread_exit(0);
	return NULL;
}

template <int dim> bool OutsideDomainCheck(MMSP::grid<dim, int>& grid, vector<int>* x){
  bool outside_domain=false;
  for(int i=0; i<dim; i++){
    if((*x)[i]<x0(grid, i) || (*x)[i]>x1(grid, i)){
      outside_domain=true;
      break;
    }
  }
  return outside_domain;
}

template <int dim> double PdenominatorMax(MMSP::grid<dim, int>& grid){
   double Pdenominator_max = 0.0;
   vector<int> coords (dim,0);
   if(dim==2){
       for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++) 
         for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++){
           coords[0] = codx;
           coords[1] = cody;
           double temperature = grid.AccessToTmp(coords);
           double Pdenominator = exp(-Q/R/temperature)/pow(grid.AccessToTmc(coords), (n*n1-1));
           if(Pdenominator > Pdenominator_max){
             Pdenominator_max = Pdenominator;
           }
         }
   }
   else if(dim==3){
     for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++) 
       for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++) 
         for(int codz=x0(grid, 2); codz <= x1(grid, 2); codz++){
           coords[0] = codx;
           coords[1] = cody;
           coords[2] = codz;
           double temperature = grid.AccessToTmp(coords);
           double Pdenominator = exp(-Q/R/temperature)/pow(grid.AccessToTmc(coords), (n*n1-1));
           if(Pdenominator > Pdenominator_max){
             Pdenominator_max = Pdenominator;
           }
         }
   }
   return Pdenominator_max;
}

template <int dim> void UpdateLocalTmc(MMSP::grid<dim, int>& grid, double t_inc){
   vector<int> coords (dim,0);
   if(dim==2){
       for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++) 
         for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++){
           coords[0] = codx;
           coords[1] = cody;
           long double exp_eqn_rhs = pow(K1*lambda*pow(grid.AccessToTmc(coords), n1), n) - pow(L0, n);
           long double temperature = grid.AccessToTmp(coords);
           exp_eqn_rhs += K*t_inc*exp(-Q/R/temperature);
           grid.AccessToTmc(coords) = pow(1.0/K1/lambda*pow(exp_eqn_rhs+pow(L0, n),1.0/n), 1.0/n1);  
         }
   }
   else if(dim==3){
     for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++)  
         for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++) 
           for(int codz=x0(grid, 2); codz <= x1(grid, 2); codz++){
             coords[0] = codx;
             coords[1] = cody;
             coords[2] = codz;
             long double exp_eqn_rhs = pow(K1*lambda*pow(grid.AccessToTmc(coords), n1), n) - pow(L0, n);
             long double temperature = grid.AccessToTmp(coords);
             exp_eqn_rhs += K*t_inc*exp(-Q/R/temperature);
             grid.AccessToTmc(coords) = pow(1.0/K1/lambda*pow(exp_eqn_rhs+pow(L0, n),1.0/n), 1.0/n1);  
           }
   }
}

template <int dim> void UpdateLocalTmp(MMSP::grid<dim, int>& grid){
   vector<int> coords (dim,0);
   double temp[2] = {673.0, 673.0}; // define temperatures at two ends of x direction 
   if(dim==2){
       for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++) 
         for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++){
           coords[0] = codx;
           coords[1] = cody;
           grid.AccessToTmp(coords) = temp[1]+(temp[0]-temp[1])/1000*codx; // 1000 is the lattice size
         }
   }
   else if(dim==3){
     for(int codx=x0(grid, 0); codx <= x1(grid, 0); codx++)
         for(int cody=x0(grid, 1); cody <= x1(grid, 1); cody++)
           for(int codz=x0(grid, 2); codz <= x1(grid, 2); codz++){
             coords[0] = codx;
             coords[1] = cody;
             coords[2] = codz;
             grid.AccessToTmp(coords) = temp[1]+(temp[0]-temp[1])/128*codx;
           }
   }
}

template <int dim> unsigned long update(MMSP::grid<dim, int>& grid, int steps, int nthreads, long double &physical_time)
{
	#if (!defined MPI_VERSION) && ((defined CCNI) || (defined BGQ))
	std::cerr<<"Error: MPI is required for CCNI."<<std::endl;
	exit(1);
	#endif
	int rank=0;
	unsigned int np=0;
	#ifdef MPI_VERSION
	rank=MPI::COMM_WORLD.Get_rank();
	np=MPI::COMM_WORLD.Get_size();
	MPI::COMM_WORLD.Barrier();
	#endif

	unsigned long update_timer = 0;
	pthread_t* p_threads = new pthread_t[nthreads];
	flip_index<dim>* mat_para = new flip_index<dim> [nthreads];
	pthread_attr_t attr;
	pthread_attr_init (&attr);

	#ifndef SILENT
	static int iterations = 1;
	if (rank==0) print_progress(0, steps, iterations);
	#endif

/*---------------generate cells------------------*/
  int dimension_length=0, number_of_lattice_cells=1;
  int lattice_cells_each_dimension[dim];
  for(int i=0; i<dim; i++){
    dimension_length = x1(grid, i)-x0(grid, i);
    if(x0(grid, 0)%2==0)
      lattice_cells_each_dimension[i] = dimension_length/2+1;
    else
      lattice_cells_each_dimension[i] = 1+(dimension_length%2==0?dimension_length/2:dimension_length/2+1);
    number_of_lattice_cells *= lattice_cells_each_dimension[i];
  }

//----------assign cells for each pthreads
  int num_of_cells_in_thread = number_of_lattice_cells/nthreads;
  //check if num of the pthread is too large, if so, reduce it.                                                                                                                                         
  if (num_of_cells_in_thread<1) {
    std::cerr<<"ERROR: number of pthread is too large, please reduce it to a value <= "<<number_of_lattice_cells<<std::endl;
    exit(0);
  }

	vector<int> x (dim,0);
	vector<int> x_prim (dim,0);
  int coordinates_of_cell[dim];
  int initial_coordinates[dim];
  int **cell_coord = new int*[nthreads];//record the start coordinates of each pthread domain.
  for(int i=0; i<nthreads; i++){
    cell_coord[i] = new int[dim];
    for(int j=0; j<dim; j++){
      cell_coord[i][j]=0;
    }
  }

  int **num_of_grids_to_flip = new int*[nthreads];
  for(int i=0; i<nthreads; i++){
    num_of_grids_to_flip[i] = new int[( static_cast<int>(pow(2,dim)) )];
    for(int j=0; j<pow(2,dim); j++){
      num_of_grids_to_flip[i][j]=0;
    }
  }

  for(int k=0; k<dim; k++) 
    initial_coordinates[k] = x0(grid, k);
  for(int i=0; i<dim; i++){
    if(x0(grid, i)%2!=0) 
      initial_coordinates[i]--;
  }

  for(int i=0; i<nthreads; i++) {
        int cell_numbering = num_of_cells_in_thread*i; //0-indexed, celling_numbering is the start cell numbering
        if(dim==2){
          cell_coord[i][dim-1]=cell_numbering%lattice_cells_each_dimension[dim-1];//0-indexed
          cell_coord[i][0]=(cell_numbering/lattice_cells_each_dimension[dim-1]);
	        if(cell_coord[i][0]>=lattice_cells_each_dimension[0]){
	          std::cerr<<"the cell coordinates is wrong!"<<std::endl;
	          exit(1);
	        }
        }else if(dim==3){
          cell_coord[i][dim-1]=cell_numbering%lattice_cells_each_dimension[dim-1];//0-indexed
          cell_coord[i][1]=(cell_numbering/lattice_cells_each_dimension[dim-1])%lattice_cells_each_dimension[1];
          cell_coord[i][0]=(cell_numbering/lattice_cells_each_dimension[dim-1])/lattice_cells_each_dimension[1];
	        if(cell_coord[i][0]>=lattice_cells_each_dimension[0]){
	          std::cerr<<"the cell coordinates is wrong!"<<std::endl;
	          exit(1);
	        }
        }

    mat_para[i].grid = &grid;
    if(i==(nthreads-1)) 
      mat_para[i].num_of_cells_in_thread = number_of_lattice_cells - num_of_cells_in_thread*(nthreads-1);
    else 
      mat_para[i].num_of_cells_in_thread = num_of_cells_in_thread;

    for(int k=0; k<dim; k++) 
      mat_para[i].lattice_cells_each_dimension[k]=lattice_cells_each_dimension[k];

    for(int j=0; j<mat_para[i].num_of_cells_in_thread; j++){
      int start_cell_numbering = num_of_cells_in_thread*i;
      if(dim==2){
        coordinates_of_cell[dim-1]=(start_cell_numbering+j)%lattice_cells_each_dimension[dim-1];//0-indexed
        coordinates_of_cell[0]=(start_cell_numbering+j)/lattice_cells_each_dimension[dim-1];
      }else if(dim==3){
        coordinates_of_cell[dim-1]=(start_cell_numbering+j)%lattice_cells_each_dimension[dim-1];//0-indexed
        coordinates_of_cell[1]=((start_cell_numbering+j)/lattice_cells_each_dimension[dim-1])%lattice_cells_each_dimension[1];
        coordinates_of_cell[0]=((start_cell_numbering+j)/lattice_cells_each_dimension[dim-1])/lattice_cells_each_dimension[1];
      }
      for(int ii=0; ii<dim; ii++){
        x[ii]=initial_coordinates[ii]+2*coordinates_of_cell[ii];
      }

      if(dim==2){
        x_prim = x;
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][0]+=1;

        x_prim = x;
        x_prim[1]=x[1]+1; //0,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][1]+=1;

        x_prim = x;
        x_prim[0]=x[0]+1; //1,0
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][2]+=1;

        x_prim = x;
        x_prim[0]=x[0]+1;
        x_prim[1]=x[1]+1; //1,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][3]+=1;
      }else if(dim==3){
        x_prim = x;//0,0,0
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][0]+=1;

        x_prim = x;
        x_prim[2]=x[2]+1; //0,0,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][1]+=1;

        x_prim = x;
        x_prim[1]=x[1]+1; //0,1,0
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][2]+=1;

        x_prim = x;
        x_prim[2]=x[2]+1;
        x_prim[1]=x[1]+1; //0,1,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][3]+=1;

        x_prim = x;
        x_prim[0]=x[0]+1; //1,0,0 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][4]+=1;

        x_prim = x;
        x_prim[2]=x[2]+1;
        x_prim[0]=x[0]+1; //1,0,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][5]+=1;

        x_prim = x;
        x_prim[1]=x[1]+1;
        x_prim[0]=x[0]+1; //1,1,0 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][6]+=1;

        x_prim = x;
        x_prim[2]=x[2]+1;
        x_prim[1]=x[1]+1;
        x_prim[0]=x[0]+1; //1,1,1 
        if(!OutsideDomainCheck<dim>(grid, &x_prim)) num_of_grids_to_flip[i][7]+=1;
      }
    }// for int j 
  }//for int i

	for (int step=0; step<steps; step++){
    double Pdenominator_max_partition = PdenominatorMax(grid); 
    MPI::COMM_WORLD.Barrier();
    double Pdenominator_max_global = 0.0;

    MPI::COMM_WORLD.Allreduce(&Pdenominator_max_partition, &Pdenominator_max_global, 1, MPI_DOUBLE, MPI_MAX);
    MPI::COMM_WORLD.Barrier();

    double t_inc = n*n1*pow(K1*lambda,n)/K/Pdenominator_max_global;

		unsigned long start = rdtsc();
    int num_of_sublattices=0;
    if(dim==2) num_of_sublattices = 4; 
    else if(dim==3) num_of_sublattices = 8;
		for (int sublattice=0; sublattice < num_of_sublattices; sublattice++) {
			for (int i=0; i!= nthreads ; i++) {
				mat_para[i].sublattice=sublattice;
				mat_para[i].num_of_points_to_flip=num_of_grids_to_flip[i][sublattice];
        mat_para[i].Pdenominator = Pdenominator_max_global;

        for(int k=0; k<dim; k++) mat_para[i].cell_coord[k]=cell_coord[i][k];
				pthread_create(&p_threads[i], &attr, flip_index_helper<dim>, (void*) &mat_para[i] );
			}//loop over threads

			for (int ii=0; ii!= nthreads ; ii++)
				pthread_join(p_threads[ii], NULL);

			#ifdef MPI_VERSION
			MPI::COMM_WORLD.Barrier();
			#endif

			ghostswap(grid, sublattice); // once looped over a "color", ghostswap.
//			ghostswap(grid);

      #ifdef MPI_VERSION
			MPI::COMM_WORLD.Barrier();
      #endif
		}//loop over color

	  MPI::COMM_WORLD.Barrier();
    physical_time += t_inc;
	  MPI::COMM_WORLD.Barrier();
    UpdateLocalTmc(grid, t_inc);
	  MPI::COMM_WORLD.Barrier();
//    UpdateLocalTmp(grid, physical_time);
	  MPI::COMM_WORLD.Barrier();

		#ifndef SILENT
		if (rank==0) print_progress(step+1, steps, iterations);
		#endif
		update_timer += rdtsc()-start;
	}//loop over step
	#ifndef SILENT
	++iterations;
	#endif

  for(int i=0; i<nthreads; i++){
    delete [] num_of_grids_to_flip[i];
    num_of_grids_to_flip[i]=NULL;
    delete [] cell_coord[i];
    cell_coord[i]=NULL;
  }
  delete num_of_grids_to_flip; 
  num_of_grids_to_flip=NULL; 
  delete cell_coord;
  cell_coord=NULL;
	delete [] p_threads;
	p_threads=NULL;
	delete [] mat_para;
	mat_para=NULL;

	unsigned long total_update_time=update_timer;
	#ifdef MPI_VERSION
	MPI::COMM_WORLD.Allreduce(&update_timer, &total_update_time, 1, MPI_UNSIGNED_LONG, MPI_SUM);
  MPI::COMM_WORLD.Barrier();
	#endif
	return total_update_time/np; // average update time
}

}

#ifndef SILENT
void print_progress(const int step, const int steps, const int iterations)
{
	char* timestring;
	static unsigned long tstart;
	struct tm* timeinfo;

	if (step==0) {
		tstart = time(NULL);
		std::time_t rawtime;
		std::time( &rawtime );
		timeinfo = std::localtime( &rawtime );
		timestring = std::asctime(timeinfo);
		timestring[std::strlen(timestring)-1] = '\0';
		std::cout<<"Pass "<<std::setw(3)<<std::right<<iterations<<": "<<timestring<<" ["<<std::flush;
	} else if (step==steps) {
		unsigned long deltat = time(NULL)-tstart;
		std::cout << "•] "
							<<std::setw(2)<<std::right<<deltat/3600<<"h:"
							<<std::setw(2)<<std::right<<(deltat%3600)/60<<"m:"
							<<std::setw(2)<<std::right<<deltat%60<<"s"
							<<" (File "<<std::setw(5)<<std::right<<iterations*steps<<")."<<std::endl;
	} else if ((20 * step) % steps == 0) std::cout<<"• "<<std::flush;
}
#endif

#endif
#include"MMSP.main.hpp"
